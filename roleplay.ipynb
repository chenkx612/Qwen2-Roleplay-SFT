{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd112e1e",
   "metadata": {},
   "source": [
    "# 中文角色扮演微调\n",
    "\n",
    "**目标**：在有限计算资源（Colab 免费 GPU）下，对 Qwen3-1.7B 做小规模 SFT，使其能根据 `instruction`（角色设定）与 `input` 进行一轮符合设定的对话并输出 `output`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18533a54",
   "metadata": {},
   "source": [
    "## 1. 准备工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c88d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93daa098",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/chenkx612/Qwen3-Roleplay-SFT.git\n",
    "%cd Qwen3-Roleplay-SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be8a35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b75c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "OUTPUT_DIR = \"./checkpoints\"\n",
    "ADAPTER_DIR = \"./adapter\"\n",
    "SEED = 42\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(ADAPTER_DIR, exist_ok=True)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bf8a13",
   "metadata": {},
   "source": [
    "## 2. 加载 LLM & Tokenizer & LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014ce864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-1.7B\"\n",
    "CACHE_DIR = \"/content/hf_cache\"  # 本地缓存目录\n",
    "USE_4BIT = True\n",
    "\n",
    "if USE_4BIT:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        trust_remote_code=True,                 # 允许执行模型 repo 的自定义代码\n",
    "        device_map=\"auto\",                      # 自动把模型切到可用设备/做分配\n",
    "        quantization_config=bnb_config          # bitsandbytes 的量化配置\n",
    "    )\n",
    "else:\n",
    "    # 优先用 fp16 在 GPU 上加载，降低显存占用（如果没有 GPU，会回退到 cpu）\n",
    "    dtype = torch.float16 if torch.cuda.is_available() else None\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        trust_remote_code=True,\n",
    "        device_map='auto',\n",
    "        torch_dtype=dtype\n",
    "    )\n",
    "\n",
    "print('Loaded model class:', model.__class__)\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e2d426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME, cache_dir=CACHE_DIR, trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469f0988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "if USE_4BIT:\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\",\n",
    "        \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c720b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印可训练参数供检查（LoRA 只激活小部分参数）\n",
    "from utils import print_trainable_parameters\n",
    "print_trainable_parameters(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08695d79",
   "metadata": {},
   "source": [
    "## 3. 加载并筛选数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3e2191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from utils import total_length\n",
    "\n",
    "TOP_N = 100\n",
    "MAX_LEN = 500\n",
    "MIN_LEN = 100\n",
    "\n",
    "raw_ds = load_dataset(\"LooksJuicy/Chinese-Roleplay-SingleTurn\")\n",
    "train_raw = raw_ds['train']\n",
    "\n",
    "def is_valid_sample(sample, min_len=MIN_LEN, max_len=MAX_LEN):\n",
    "    total_len = total_length(sample)\n",
    "    if total_len < min_len or total_len > max_len:\n",
    "        return False\n",
    "    if (\n",
    "        not sample['instruction'].strip() or \n",
    "        not sample['input'].strip() or \n",
    "        not sample['output'].strip()\n",
    "    ):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "filtered_samples = [s for s in train_raw if is_valid_sample(s)]\n",
    "filtered_samples = sorted(filtered_samples, key=total_length, reverse=True)\n",
    "train_ds = Dataset.from_list(\n",
    "    filtered_samples[:min(TOP_N, len(filtered_samples))]\n",
    ")\n",
    "print(f\"筛选后样本数: {len(train_ds)}\")\n",
    "print(train_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12993a79",
   "metadata": {},
   "source": [
    "## 4. Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d5a30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_roleplay(example, include_assistant=True):\n",
    "    \"\"\"\n",
    "    将 instruction 作为角色设定放入 system, input 作为 user, output 作为 assistant.\n",
    "    include_assistant: 当为 False 时，会省略 assistant 参考答案（用于推理/测试）\n",
    "\n",
    "    返回值: dict, 包含 'full_text'（用于 tokenization 的完整对话文本）和\n",
    "    'assistant_text' (参考答案，仅训练时用于生成 labels) 。\n",
    "    \"\"\"\n",
    "    instr = example.get(\"instruction\", \"\").strip() or \"<未提供角色设定>\"\n",
    "    user_input = example.get(\"input\", \"\").strip() or \"<无用户输入>\"\n",
    "    assistant_output = example.get(\"output\", \"\").strip() or \"\"\n",
    "\n",
    "    system_prompt = (\n",
    "        \"你将扮演由下方“角色设定”描述的角色。\"\n",
    "        \"始终以该角色的第一人称身份回答，保持角色的语气、知识和情感一致，增强代入感。\"\n",
    "        f\"\\n\\n角色设定：\\n{instr}\\n\\n\"\n",
    "    )\n",
    "\n",
    "    def _build_conversation(include_assistant_flag: bool):\n",
    "        conv = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_input},\n",
    "        ]\n",
    "        if include_assistant_flag and assistant_output:\n",
    "            conv.append({\"role\": \"assistant\", \"content\": assistant_output})\n",
    "        return conv\n",
    "\n",
    "    full_text = tokenizer.apply_chat_template(\n",
    "        conversation=_build_conversation(include_assistant), tokenize=False\n",
    "    )\n",
    "\n",
    "    return {\"full_text\": full_text, \"assistant_text\": assistant_output}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d815204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(format_roleplay, remove_columns=train_ds.column_names)\n",
    "print(train_ds[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415f9d79",
   "metadata": {},
   "source": [
    "## 5. 微调前测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a12eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入 samples.json 并对每个样例在微调前进行一次推理，保存结果到 pre_results\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "MAX_LENGTH = 1024\n",
    "DO_SAMPLE = True\n",
    "DECODE_TEMPERATURE = 1.0\n",
    "DECODE_TOP_K = 50\n",
    "DECODE_TOP_P = 0.9\n",
    "DECODE_MAX_NEW_TOKENS = 256\n",
    "\n",
    "samples_path = Path('samples.json')\n",
    "if not samples_path.exists():\n",
    "    raise FileNotFoundError(f'samples.json not found at {samples_path.resolve()}')\n",
    "samples = json.loads(samples_path.read_text(encoding='utf-8'))\n",
    "\n",
    "# 复用统一的 format_roleplay，但在测试/推理时不包含参考答案\n",
    "def build_prompt_from_sample(s):\n",
    "    out = format_roleplay(s, include_assistant=False)\n",
    "    return out['full_text'] if isinstance(out, dict) else out\n",
    "\n",
    "def generate_for_prompt(prompt):\n",
    "    # 批量/动态填充并移动到模型所在设备\n",
    "    inputs = tokenizer(\n",
    "        prompt, return_tensors='pt', truncation=True, \n",
    "        padding=True, max_length=MAX_LENGTH\n",
    "    ).to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        gen = model.generate(\n",
    "            **inputs, max_new_tokens=DECODE_MAX_NEW_TOKENS,\n",
    "            do_sample=DO_SAMPLE, temperature=DECODE_TEMPERATURE,\n",
    "            top_k=DECODE_TOP_K, top_p=DECODE_TOP_P\n",
    "        )\n",
    "    text = tokenizer.batch_decode(gen, skip_special_tokens=True)[0]\n",
    "    return text\n",
    "\n",
    "pre_results = []\n",
    "for s in samples:\n",
    "    prompt = build_prompt_from_sample(s)\n",
    "    out = generate_for_prompt(prompt)\n",
    "    pre_results.append(out)\n",
    "\n",
    "print('微调前测试完成，样本数：', len(pre_results))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791d81c0",
   "metadata": {},
   "source": [
    "## 6. 微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e0c975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, default_data_collator\n",
    "from utils import find_sublist\n",
    "\n",
    "PER_DEVICE_BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "NUM_EPOCHS = 2\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "# 查看 train_ds 的列，选择第一个字符串列作为文本来源（对不同数据结构做兼容处理）\n",
    "print('train_ds columns:', train_ds.column_names)\n",
    "\n",
    "def _get_text_list(batch):\n",
    "    # batch 是一个 dict，值是 list。寻找 'full_text' 列优先，其次回退到第一个字符串列\n",
    "    if 'full_text' in batch:\n",
    "        return batch['full_text']\n",
    "    for k, v in batch.items():\n",
    "        if isinstance(v, list) and v and isinstance(v[0], str):\n",
    "            return v\n",
    "    # 兜底：把第一个列强转为字符串列表\n",
    "    first = next(iter(batch.items()))\n",
    "    return [str(x) for x in first[1]]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # texts: 已包含 assistant（训练时）或不包含（用于推理时）\n",
    "    texts = _get_text_list(examples)\n",
    "    tokenized = tokenizer(\n",
    "        texts, truncation=True, padding='max_length', max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "    # 获取对应的 assistant_text 列（可能为空字符串列表）\n",
    "    assistant_texts = examples.get('assistant_text', [''] * len(texts))\n",
    "    # tokenize assistant_texts without special tokens to get token ids sequence\n",
    "    assistant_tokenized = tokenizer(assistant_texts, add_special_tokens=False).input_ids\n",
    "\n",
    "    pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "\n",
    "    labels = []\n",
    "    for input_ids, assist_ids in zip(tokenized['input_ids'], assistant_tokenized):\n",
    "        # 默认全部 -100\n",
    "        lab = [-100] * len(input_ids)\n",
    "        if assist_ids:\n",
    "            # 在 full input_ids 中寻找 assist_ids 子序列\n",
    "            start = find_sublist(input_ids, assist_ids)\n",
    "            if start != -1:\n",
    "                for i in range(start, start + len(assist_ids)):\n",
    "                    if i < len(lab):\n",
    "                        lab[i] = input_ids[i]\n",
    "            else:\n",
    "                # 未找到时：尝试在去掉 padding 后的末尾区域对齐\n",
    "                real_len = len(input_ids)\n",
    "                while real_len > 0 and input_ids[real_len-1] == pad_id:\n",
    "                    real_len -= 1\n",
    "                start = max(0, real_len - len(assist_ids))\n",
    "                for i in range(start, real_len):\n",
    "                    lab[i] = input_ids[i]\n",
    "        # else assistant 为空，保持全 -100\n",
    "        labels.append(lab)\n",
    "\n",
    "    tokenized['labels'] = labels\n",
    "    return tokenized\n",
    "\n",
    "# tokenized_ds 用于训练。remove_columns 保留 label/input_ids，不会丢失需要的列\n",
    "tokenized_ds = train_ds.map(\n",
    "    tokenize_function, batched=True, remove_columns=train_ds.column_names\n",
    ")\n",
    "print('tokenized_ds example:', tokenized_ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17bd507",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./checkpoints',\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    save_strategy='epoch',\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\"  # 关闭wandb日志\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds,\n",
    "    data_collator=default_data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "train_result = trainer.train()\n",
    "print('train_result:', train_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7820bc4",
   "metadata": {},
   "source": [
    "## 7. 微调后测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0968add3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对 samples.json 再次推理，收集 post_results 并与 pre_results 并列展示\n",
    "post_results = []\n",
    "for s in samples:\n",
    "    prompt = build_prompt_from_sample(s)\n",
    "    out = generate_for_prompt(prompt)\n",
    "    post_results.append(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83ea4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "table_data = []\n",
    "for i, sample in enumerate(samples):\n",
    "    table_data.append({\n",
    "        '角色': sample['name'],\n",
    "        '输入': sample['input'],\n",
    "        '微调前': pre_results[i] if i < len(pre_results) else '',\n",
    "        '微调后': post_results[i] if i < len(post_results) else ''\n",
    "    })\n",
    "df = pd.DataFrame(table_data, columns=['角色','输入','微调前','微调后'])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c4fdbf",
   "metadata": {},
   "source": [
    "## 8. 保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2338e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# 仅保存 LoRA adapter（不保存基模型或 tokenizer），以节省存储空间。\n",
    "# model 是经过 get_peft_model 包装的 PeftModel，save_pretrained 仅会保存 adapter 权重和配置。\n",
    "model.save_pretrained(ADAPTER_DIR)\n",
    "print('\\n保存 LoRA adapter 至:', ADAPTER_DIR)\n",
    "\n",
    "# 把保存的文件打包，方便下载\n",
    "archive_path = shutil.make_archive(ADAPTER_DIR, 'zip', ADAPTER_DIR)\n",
    "print('\\n已将 adapter 打包为:', archive_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
